## 数据处理方法

#### 天科院原始数据处理方法

刚开始的时候是通过统计词数出现的频率来决定选取哪些特征，现在特征数据已经提取完成就可以直接使用大语言模型来清洗数据

```json
"塞拉利昂": {
        "援塞拉利昂渔码头项目": {
            "地理位置": {
                "城市": "弗里敦",
                "纬度": "8.4844°N",
                "经度": "13.2344°W"
            },
            "水文特征": {
                "水深": "最大水深约为15m，平均水深约为8m",
                "潮汐特征": "最大潮差2.69m，最小潮差0.70m；最高潮位1.39m，最低潮位-1.37m；平均涨潮历时6小时9分，落潮历时6小时15分。",
                "潮流": "测点最大流速：涨潮流1.35m/s，落潮流0.44m/s；流向主要为东南至西北，NE至NNE（涨潮），SW至SSW（落潮）。"
            },
            "波浪特征": {
                "波高": "有效波高2.88m（50年一遇），2.34m（10年一遇）；最大有效波高1.43m，平均波高0.53m。",
                "波周期": "平均波周期16.64s（50年一遇），14.96s（10年一遇）；平均周期4.5s，最大周期6.7s。",
                "波向": "主要波向为西南；常浪向WSW，强浪向W。"
            },
            "流场特征": {
                "流速": "设计流速0.7m/s，最大流速1.1m/s；大潮期间最大流速0.34-1.62m/s，流速变化范围较大。",
                "流向": "主要流向为西南至东南；涨潮流主要方向为NE，落潮流主要方向为SW。"
            },
            "泥沙特征": {
                "含沙量": "大潮期间含沙量范围0.005-0.017kg/m³，小潮期间含沙量范围0.003-0.014kg/m³。",
                "泥沙运输特性": "渔码头周围淤积主要由近岸流带来的泥沙所致，预计淤积量为每年0.05m/a。",
                "底质颗粒分析": "表层沉积物以粉砂、粘土质粉砂、砂质粘土为主，中值粒径21.42-190.52um。",
                "泥沙沉降特性": "无具体数据"
            },
            "风特征": {
                "风速": "年平均风速为4.5m/s，最大风速为11.5m/s；测量期间平均风速最大为9.0m/s，阵风风速最大为12.2m/s。",
                "风向": "常见风向为西南风；主导风向为NW，占比68.19%。"
            }
        }
    }
```

​	例如上述数据，将其发送给大语言模型(之后均用GPT代称)，然后让其针对上传的文件清洗数据即可。如果缺失太多数据的项目，我会选择不放入软件中，但是为了数据完整性后续也可以放入其中；对于一个项目多个文件的，直接上传多个文件让GPT进行数据清洗。清洗完之后只要遵行json文件格式放入对应的data.json文件里面即可。

#### countries_states_cities处理方式

由于翻译原因，还有天科院给的数据中有中文有英文，所以需要手动处理城市索引功能，因为有些时候储存的是中文城市名(国外)，但是由于countries_states_cities里面外国均基本均用西班牙或英语，所以需要手动翻译来处理这个麻烦。



## AI大模型的微调

#### prompt

我的prompt如下，其实多一点少一点影响不大，最重要的是要明确

```
已知信息：
{data}
回答要求：
- 请使用简洁且专业的语言来回答用户的问题。
- 如果你不知道答案，请回答“没有在知识库中查找到相关信息，建议咨询相关技术支持或参考官方文档进行操作”。
- 避免提及你是从已知信息中获得的知识。
- 请保证答案与已知信息中描述的一致。
- 请使用 Markdown 语法优化答案的格式。
- 已知信息中的图片、链接地址和脚本语言请直接返回。
- 请使用与问题相同的语言来回答。
- 尽可能使得回答模式比较自然
- 回答精炼，避免无意义的报备内容和解释内容。
- 请完整引用知识库中的内容。
- 只需要聚焦于所询问的知识库条目，不需要参考其他知识库条目。
- 遇到分段内容时，不要提前终止引用，请确保引用段落的完整性。
- 遇到长内容时，可以分多个段落引用，但每个段落必须保持完整。
- 如果提问内容与知识库相匹配达到一定的几率，直接整理并输出知识库的内容
- 如果有明确的提问，只需要知识库条目引用，不需要过多处理
- 只需要简洁准确的回答提出的问题，避免无意义的修饰和说明
问题：
{question}
```

#### AI大模型的调用

【1700多种开源大模型随意部署！一键本地搭建大模型+知识库，不挑环境、不挑配置】 https://www.bilibili.com/video/BV1bU411Z7eo/?share_source=copy_web&vd_source=abd4fbfdcb23952232373fff033321b2

上述链接详细讲述了配置过程，所需的就是大语言模型的选取，之前尝试了Qwne: 4b，效果奇差；之后使用了QWen: 7b，效果比较好，但是需要3050Ti及以上显卡能比较流畅的完成任务；Qwen2: 72b是性能最好的，但是需要4090Ti才能带的动，如果真的需要可以云端租主机。

#### AI大模型

![image-20240830115517931](D:/Typora/typora_photo/image-20240830115517931.png)

![image-20240830115533574](D:/Typora/typora_photo/image-20240830115533574.png)

关于知识库的构建，是十分重要的，最关键就是分段一定要清晰，分段越多越清晰，效果越好，具体可以交由GPT进行处理，但是第一次的数据清洗和分段还是自己来比较好，能够符合自己的需求



**以上基本就是数据处理方法和AI微调处理方法，好的AI模型比任何微调都有效，还是找一个好的模型最关键**